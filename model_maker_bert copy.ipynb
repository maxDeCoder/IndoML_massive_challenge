{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "# remove links\n",
    "# remove punctuation\n",
    "# remove hashtags\n",
    "\n",
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = strip_all_entities(strip_links(text))\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = \" \".join([word for word in text if word not in nltk.corpus.stopwords.words('english')])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Data_English.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['English'].apply(preprocess)\n",
    "#train_labels = train_df[\"target\"]\n",
    "\n",
    "del [train_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 40\n",
    "num_samples = len(train_texts)\n",
    "\n",
    "Xids = np.zeros((num_samples, seq_len))\n",
    "Xmask = np.zeros((num_samples, seq_len))\n",
    "\n",
    "#labels = train_labels.to_numpy()\n",
    "#labels = np.expand_dims(labels, axis=0).T\n",
    "\n",
    "Xids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "for i, phrase in enumerate(tqdm(train_texts)):\n",
    "    token = tokenizer.encode_plus(\n",
    "        phrase, max_length=seq_len, add_special_tokens=True, \n",
    "        padding=\"max_length\", truncation=True, return_tensors='tf')\n",
    "\n",
    "    Xids[i, :] = token['input_ids']\n",
    "    Xmask[i, :] = token['attention_mask']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(inputs_ids, masks):\n",
    "    return {\n",
    "        'input_ids': inputs_ids,\n",
    "        'attention_mask': masks\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 8\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask))\n",
    "dataset = dataset.map(map_func)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "# pretrained bert weights\n",
    "bert = TFAutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# make untrainable\n",
    "bert.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = keras.layers.Input(shape=(seq_len,), name=\"input_ids\", dtype=\"int32\")\n",
    "attention_mask = keras.layers.Input(shape=(seq_len,), name=\"attention_mask\", dtype=\"int32\")\n",
    "\n",
    "# encode meaning of sentence\n",
    "embeddings = bert.bert(input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "#x = layers.Dense(1024, activation=\"relu\")(embeddings)\n",
    "#x = layers.Dropout(0.5)(x)\n",
    "#x = layers.Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[input_ids, attention_mask], outputs=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer= 'adam',\n",
    "    loss='binary_crossentropy',\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl.dump(outputs, open(\"embeddings.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"embeddings\"] = outputs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_embedding(embeddings):\n",
    "    embeddings = np.array(embeddings)\n",
    "    user_embeddings = np.sum(embeddings, axis=0)\n",
    "    return user_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_embedding(train_df[train_df[\"Username\"]==\"000kiran_\"][\"embeddings\"].to_list()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(train_df[\"Username\"].unique()), 768))\n",
    "\n",
    "for i, username in enumerate(tqdm(train_df[\"Username\"].unique())):\n",
    "    embeddings[i, :] = mode_embedding(train_df[train_df[\"Username\"]==username][\"embeddings\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(embeddings, open(\"user embeddings.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "doc_embeddings = pkl.load(open(\"embeddings.pkl\", \"rb\"))\n",
    "user_embeddings = pkl.load(open(\"user embeddings.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(user_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"username\":train_df[\"Username\"].unique(), \"cluster\":kmeans.labels_}).to_csv(\"Clustered users.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  # to standardize the features\n",
    "from sklearn.decomposition import PCA  # to apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_clusters = pd.read_csv(\"Clustered users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "# doc_embeddings = pkl.load(open(\"embeddings.pkl\", \"rb\"))\n",
    "user_embeddings = pkl.load(open(\"user embeddings.pkl\", \"rb\"))\n",
    "user_clusters = pd.read_csv(\"Clustered users.csv\")\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_data = pd.DataFrame(scalar.fit_transform(user_embeddings)) #scaling the data\n",
    "# scaled_data = user_embeddings\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(scaled_data)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "reduced = pca.fit_transform(scaled_data)\n",
    "\n",
    "# We need a 2 x 944 array, not 944 by 2 (all X coordinates in one list)\n",
    "t = reduced.transpose()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(t[0, :], t[1, :], t[2, :], c=clusters, cmap=\"viridis\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(scaled_data)\n",
    "\n",
    "# We need a 2 x 944 array, not 944 by 2 (all X coordinates in one list)\n",
    "t = reduced.transpose()\n",
    "\n",
    "plt.scatter(t[0], t[1], c=clusters)\n",
    "plt.show()\n",
    "\n",
    "print(\"silhouette score:\", silhouette_score(scaled_data, kmeans.predict(scaled_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"Preprocess data for KMeans clustering\"\"\"\n",
    "    \n",
    "    # df_log = np.log1p(df)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    return scaler.fit_transform(df)\n",
    "\n",
    "def find_k(df, increment=0, decrement=0):\n",
    "    \"\"\"Find the optimum k clusters\"\"\"\n",
    "    \n",
    "    # df_norm = preprocess(df)\n",
    "    df_norm = df\n",
    "    sse = {}\n",
    "    \n",
    "    for k in tqdm(list(range(1, 10))):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "        kmeans.fit(df_norm)\n",
    "        sse[k] = kmeans.inertia_\n",
    "    \n",
    "    kn = KneeLocator(x=list(sse.keys()), \n",
    "                 y=list(sse.values()), \n",
    "                 curve='convex', \n",
    "                 direction='decreasing')\n",
    "    k = kn.knee + increment - decrement\n",
    "    return k\n",
    "\n",
    "doc_embeddings = pkl.load(open(\"embeddings.pkl\", \"rb\"))\n",
    "user_clusters = pd.read_csv(\"Clustered users.csv\")\n",
    "doc_clusters = pd.read_csv(\"Data_English.csv\")\n",
    "\n",
    "num_clusters = find_k(doc_embeddings)\n",
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "doc_embeddings = pkl.load(open(\"embeddings.pkl\", \"rb\"))\n",
    "user_clusters = pd.read_csv(\"Clustered users.csv\")\n",
    "doc_clusters = pd.read_csv(\"Data_English.csv\")\n",
    "\n",
    "scalar = StandardScaler()\n",
    "# scaled_data = pd.DataFrame(scalar.fit_transform(doc_embeddings)) #scaling the data\n",
    "scaled_data = doc_embeddings\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=\"auto\").fit(scaled_data)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "reduced = pca.fit_transform(scaled_data)\n",
    "\n",
    "# We need a 2 x 944 array, not 944 by 2 (all X coordinates in one list)\n",
    "t = reduced.transpose()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(t[0, :], t[1, :], t[2, :], c=clusters, cmap=\"viridis\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(scaled_data)\n",
    "\n",
    "# We need a 2 x 944 array, not 944 by 2 (all X coordinates in one list)\n",
    "t = reduced.transpose()\n",
    "\n",
    "plt.scatter(t[0], t[1], c=clusters)\n",
    "plt.show()\n",
    "\n",
    "print(\"silhouette score:\", silhouette_score(scaled_data, kmeans.predict(scaled_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clusters = pd.read_csv(\"Data_English.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clusters[\"clusters\"] = clusters\n",
    "doc_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "\n",
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = strip_all_entities(strip_links(text))\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = preprocess(text)\n",
    "\n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = tokens#[word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "def get_word_count(text_list):\n",
    "    word_count = Counter(text_list)\n",
    "    return word_count\n",
    "\n",
    "for i in range(3):\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = doc_clusters[doc_clusters[\"clusters\"] == i]\n",
    "\n",
    "    # Assuming the column with the texts is named 'text_column'\n",
    "    texts = df['English']\n",
    "\n",
    "    # Preprocess the texts\n",
    "    preprocessed_texts = [preprocess_text(item) for item in tqdm(texts, f\"preprocess cluster {i}\")]\n",
    "\n",
    "    # Flatten the list of preprocessed tokens\n",
    "    all_tokens = [token for sublist in preprocessed_texts for token in sublist]\n",
    "\n",
    "    # Get the word count\n",
    "    word_count = get_word_count(all_tokens)\n",
    "\n",
    "    # Convert word_count dictionary to a DataFrame\n",
    "    word_count_df = pd.DataFrame(list(word_count.items()), columns=['Word', 'Count'])\n",
    "\n",
    "    # Save word count DataFrame to a CSV file\n",
    "    word_count_df.to_csv(f'word_count_cluster_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = doc_clusters[doc_clusters[\"clusters\"] == i]\n",
    "\n",
    "# Assuming the column with the texts is named 'text_column'\n",
    "texts = df['English']\n",
    "\n",
    "# Preprocess the texts\n",
    "preprocessed_texts = [preprocess_text(item) for item in tqdm(texts, f\"preprocess cluster {i}\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from kneed import KneeLocator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"Preprocess data for KMeans clustering\"\"\"\n",
    "    \n",
    "    # df_log = np.log1p(df)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    return scaler.fit_transform(df)\n",
    "\n",
    "def find_k(df, increment=0, decrement=0):\n",
    "    \"\"\"Find the optimum k clusters\"\"\"\n",
    "    \n",
    "    # df_norm = preprocess(df)\n",
    "    df_norm = df\n",
    "    sse = {}\n",
    "    \n",
    "    for k in tqdm(list(range(1, 10))):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "        kmeans.fit(df_norm)\n",
    "        sse[k] = kmeans.inertia_\n",
    "    \n",
    "    kn = KneeLocator(x=list(sse.keys()), \n",
    "                 y=list(sse.values()), \n",
    "                 curve='convex', \n",
    "                 direction='decreasing')\n",
    "    k = kn.knee + increment - decrement\n",
    "    return k\n",
    "\n",
    "doc_embeddings = pkl.load(open(\"embeddings.pkl\", \"rb\"))\n",
    "user_clusters = pd.read_csv(\"Clustered users.csv\")\n",
    "doc_clusters = pd.read_csv(\"Data_English.csv\")\n",
    "\n",
    "num_clusters = find_k(doc_embeddings)\n",
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle as pkl\n",
    "doc_embeddings = pkl.load(open(\"embeddings.pkl\", \"rb\"))\n",
    "# user_clusters = pd.read_csv(\"Clustered users.csv\")\n",
    "# doc_clusters = pd.read_csv(\"Data_English.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [doc_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters= 3).fit(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle as pkl\n",
    "doc_embeddings = pkl.load(open(\"embeddings.pkl\", \"rb\"))\n",
    "# user_clusters = pd.read_csv(\"Clustered users.csv\")\n",
    "# doc_clusters = pd.read_csv(\"Data_English.csv\")\n",
    "model = DBSCAN(eps=0.01, min_samples=1000, metric=\"cosine\").fit(doc_embeddings)\n",
    "clusters = model.labels_\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "reduced = pca.fit_transform(doc_embeddings)\n",
    "\n",
    "# We need a 2 x 944 array, not 944 by 2 (all X coordinates in one list)\n",
    "t = reduced.transpose()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(t[0, :], t[1, :], t[2, :], c=clusters, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = doc_embeddings\n",
    "length = np.sqrt((X**2).sum(axis=1))[:,None]\n",
    "X = X / length\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "reduced = pca.fit_transform(X)\n",
    "\n",
    "# We need a 2 x 944 array, not 944 by 2 (all X coordinates in one list)\n",
    "t = reduced.transpose()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(t[0, :], t[1, :], t[2, :], c=clusters, cmap=\"viridis\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(X)\n",
    "\n",
    "# We need a 2 x 944 array, not 944 by 2 (all X coordinates in one list)\n",
    "t = reduced.transpose()\n",
    "\n",
    "plt.scatter(t[0], t[1], c=clusters)\n",
    "plt.show()\n",
    "\n",
    "print(\"silhouette score:\", silhouette_score(X, kmeans.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "063da5ec96525b703b2b4b88ba5015678e29341c0c783b18b72decb99d23a1d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
